{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# configure bootstrap\n",
    "N_ITERATIONS = 100\n",
    "ALPHA = .95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the quora question (sampled) dataset with 1000 question pairs\n",
    "QUORA_DATA = 'data/quora_questions.csv'\n",
    "N_QUESTIONS = 1000\n",
    "\n",
    "df = pd.read_csv(QUORA_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 4)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                          question1  \\\n",
      "0      250366  What are the tips for clearing Google Summer o...   \n",
      "1      112801    How does social security rule monocular vision?   \n",
      "2       13679  Which AMD FX series laptop is equal to Intel i...   \n",
      "3      207849                  What is an addictive personality?   \n",
      "4      171197  What are the most critical metrics To measure ...   \n",
      "\n",
      "                                           question2  is_duplicate  \n",
      "0        How can I crack GSOC-Google Summer of Code?             1  \n",
      "1  Will you be approved for social security with ...             1  \n",
      "2                     Which is better: AMD FX vs i5?             0  \n",
      "3               What is a non-addictive personality?             0  \n",
      "4  What instrumental does Dr. Dre use in his comm...             0  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "356\n"
     ]
    }
   ],
   "source": [
    "n_sim = df['is_duplicate'].sum()\n",
    "print(n_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define our vector similarity scoring function based on cosine similarity\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def score(vector1, vector2, threshold):\n",
    "    sim_score = 1 if cosine_similarity([vector1],[vector2])[0,0] >= threshold else 0\n",
    "    return sim_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bootstrap accuracy confidence interval and compute average accuracy\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "def bootstrap_conf(vectors1, vectors2, is_similar, threshold, n_iterations, alpha):\n",
    "    # compute the bootstrap samples\n",
    "    n = vectors1.shape[0]\n",
    "    print('Computing bootstrap samples: ', end='')\n",
    "    stats = []\n",
    "    for i in range(n_iterations):\n",
    "        print(i,end=\" \")\n",
    "        ix_sample = resample([ix for ix in range(n)],\n",
    "                             replace=True,\n",
    "                             n_samples=n)\n",
    "        errors = 0\n",
    "        for j in ix_sample:\n",
    "            v1 = vectors1.loc[j,:]\n",
    "            v2 = vectors2.loc[j,:]\n",
    "            sim_score = score(v1, v2, threshold)\n",
    "            if sim_score != is_similar[j]:\n",
    "                errors += 1\n",
    "\n",
    "        acc = 1 - errors/n\n",
    "\n",
    "        # save the score for this bootstrap sample\n",
    "        stats.append(acc)\n",
    "\n",
    "    print('')\n",
    "    # confidence intervals\n",
    "    p = ((1.0-alpha)/2.0) * 100\n",
    "    lower = max(0.0, np.percentile(stats, p))\n",
    "    p = (alpha+((1.0-alpha)/2.0)) * 100\n",
    "    upper = min(1.0, np.percentile(stats, p))\n",
    "    # average performance\n",
    "    avg_acc = sum(stats)/n_iterations\n",
    "    \n",
    "    return lower, upper, avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# grab our data for processing\n",
    "questions1 = list(df['question1'])\n",
    "questions2 = list(df['question2'])\n",
    "is_similar = list(df['is_duplicate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntactic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build a vocabulary based on the first set of questions\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "questions = questions1.copy()\n",
    "vocab_vectorizer = CountVectorizer(analyzer='word',\n",
    "                                   binary=True,\n",
    "                                   min_df=2,\n",
    "                                   stop_words='english')\n",
    "vocab_vectorizer.fit(questions)\n",
    "vocab = list(vocab_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "798\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build docterm for questions1\n",
    "q1_vectorizer = CountVectorizer(analyzer='word', \n",
    "                                binary = True, \n",
    "                                vocabulary=vocab) # use our vocabulary\n",
    "q1_docarray = q1_vectorizer.fit_transform(questions1).toarray()\n",
    "q1_docterm = pd.DataFrame(q1_docarray, columns=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build docterm for questions2\n",
    "q2_vectorizer = CountVectorizer(analyzer='word', \n",
    "                                binary = True, \n",
    "                                vocabulary=vocab) # use our vocabulary\n",
    "q2_docarray = q2_vectorizer.fit_transform(questions2).toarray()\n",
    "q2_docterm = pd.DataFrame(q2_docarray, columns=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 798)\n",
      "(1000, 798)\n"
     ]
    }
   ],
   "source": [
    "print(q1_docterm.shape)\n",
    "print(q2_docterm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# experimentally determined similarity threshold for syntactic features\n",
    "sim_threshold = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 65.50%\n"
     ]
    }
   ],
   "source": [
    "# compute the accuracy for all N_QUESTIONS question pairs\n",
    "errors = 0\n",
    "for i in range(N_QUESTIONS):\n",
    "    v1 = q1_docterm.loc[i,:]\n",
    "    v2 = q2_docterm.loc[i,:]\n",
    "    sim_score = score(v1, v2, sim_threshold)\n",
    "    if sim_score != is_similar[i]:\n",
    "        errors += 1\n",
    "\n",
    "acc = 1 - errors/N_QUESTIONS\n",
    "\n",
    "print(\"Accuracy: {:.2f}%\".format(acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing bootstrap samples: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 \n",
      "95.00% confidence interval 62.25% and 68.05%\n",
      "Average accuracy: 65.46%\n"
     ]
    }
   ],
   "source": [
    "# accuracy confidence intervals and average accuracy\n",
    "\n",
    "lb, ub, ave_acc = bootstrap_conf(q1_docterm, \n",
    "                                 q2_docterm, \n",
    "                                 is_similar, \n",
    "                                 sim_threshold, \n",
    "                                 N_ITERATIONS, \n",
    "                                 ALPHA)\n",
    "print('{:.2f}% confidence interval {:.2f}% and {:.2f}%'.format(ALPHA*100, lb*100, ub*100))\n",
    "print(\"Average accuracy: {:.2f}%\".format(ave_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Spacy semantic model\n",
    "\n",
    "import spacy\n",
    "\n",
    "# NOTE: for performance reasons disable everything in the pipeline except the tokenizer\n",
    "nlp = spacy.load('en_core_web_lg', disable=['parser', 'tagger', 'ner', 'textcat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "# process questions for semantic features -- compute embedding vectors for question texts\n",
    "\n",
    "def embed(X):\n",
    "    '''\n",
    "    x is a list of strings and embed will compute\n",
    "    an embedding vector for each and return an array\n",
    "    of shape (len(x),EMBEDDING_DIM)\n",
    "    '''\n",
    "    vectors = []\n",
    "    text_array = np.array(X)\n",
    "\n",
    "    print(text_array.shape)\n",
    "    \n",
    "    for i in range(text_array.shape[0]):\n",
    "        vector = nlp(str(text_array[i])).vector\n",
    "        vectors.append(vector)\n",
    "    \n",
    "    return pd.DataFrame(vectors)\n",
    "\n",
    "vectors1 = embed(questions1)\n",
    "vectors2 = embed(questions2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# experimentally determined similarity threshold for semantic features\n",
    "sim_threshold = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 69.00%\n"
     ]
    }
   ],
   "source": [
    "# compute the accuracy for all N_QUESTIONS question pairs\n",
    "errors = 0\n",
    "for i in range(N_QUESTIONS):\n",
    "    v1 = vectors1.loc[i,:]\n",
    "    v2 = vectors2.loc[i,:]\n",
    "    sim_score = score(v1, v2, sim_threshold)\n",
    "    if sim_score != is_similar[i]:\n",
    "        errors += 1\n",
    "acc = 1 - errors/N_QUESTIONS\n",
    "\n",
    "print(\"Accuracy: {:.2f}%\".format(acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing bootstrap samples: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 \n",
      "95.00% confidence interval 66.24% and 71.66%\n",
      "Average accuracy: 68.98%\n"
     ]
    }
   ],
   "source": [
    "# accuracy confidence intervals and average accuracy\n",
    "\n",
    "lb, ub, ave_acc = bootstrap_conf(vectors1, \n",
    "                                 vectors2, \n",
    "                                 is_similar, \n",
    "                                 sim_threshold, \n",
    "                                 N_ITERATIONS, \n",
    "                                 ALPHA)\n",
    "print('{:.2f}% confidence interval {:.2f}% and {:.2f}%'.format(ALPHA*100, lb*100, ub*100))\n",
    "print(\"Average accuracy: {:.2f}%\".format(ave_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
