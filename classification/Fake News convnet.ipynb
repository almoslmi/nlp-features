{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "TEXT_DATA = 'data/fake_or_real_news.csv'\n",
    "GLOVE_DATA = 'data/glove.6B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a function that allows us to evaluate our models\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def evaluate_model(predict_fun, X_train, y_train, X_test, y_test):\n",
    "    '''\n",
    "    evaluate the model, both training and testing errors are reported\n",
    "    '''\n",
    "    # training error\n",
    "    y_predict_train = predict_fun(X_train)\n",
    "    print(\"Training Accuracy: {: 6.2f}%\".format(accuracy_score(y_train,y_predict_train)*100))\n",
    "    # testing error\n",
    "    y_predict_test = predict_fun(X_test)\n",
    "    print(\"Testing Accuracy: {: 6.2f}%\".format(accuracy_score(y_test,y_predict_test)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Indexing word vectors.\n",
    "# build index mapping words in the embeddings set to their embedding vector \n",
    "\n",
    "embeddings_index = {}\n",
    "with open(GLOVE_DATA) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found empty text @ 106...dropping\n",
      "found empty text @ 710...dropping\n",
      "found empty text @ 806...dropping\n",
      "found empty text @ 919...dropping\n",
      "found empty text @ 940...dropping\n",
      "found empty text @ 1664...dropping\n",
      "found empty text @ 1736...dropping\n",
      "found empty text @ 1851...dropping\n",
      "found empty text @ 1883...dropping\n",
      "found empty text @ 1941...dropping\n",
      "found empty text @ 2244...dropping\n",
      "found empty text @ 2426...dropping\n",
      "found empty text @ 2576...dropping\n",
      "found empty text @ 2662...dropping\n",
      "found empty text @ 2788...dropping\n",
      "found empty text @ 2832...dropping\n",
      "found empty text @ 3073...dropping\n",
      "found empty text @ 3350...dropping\n",
      "found empty text @ 3511...dropping\n",
      "found empty text @ 3641...dropping\n",
      "found empty text @ 3642...dropping\n",
      "found empty text @ 4014...dropping\n",
      "found empty text @ 4142...dropping\n",
      "found empty text @ 4253...dropping\n",
      "found empty text @ 4713...dropping\n",
      "found empty text @ 4744...dropping\n",
      "found empty text @ 5017...dropping\n",
      "found empty text @ 5088...dropping\n",
      "found empty text @ 5213...dropping\n",
      "found empty text @ 5581...dropping\n",
      "found empty text @ 5639...dropping\n",
      "found empty text @ 5699...dropping\n",
      "found empty text @ 5772...dropping\n",
      "found empty text @ 6064...dropping\n",
      "found empty text @ 6175...dropping\n",
      "found empty text @ 6328...dropping\n"
     ]
    }
   ],
   "source": [
    "# Processing text dataset\n",
    "# NOTE: the data file contains empty 'text' entries\n",
    "\n",
    "df = pd.read_csv(TEXT_DATA)\n",
    "df.drop(labels=['id'], axis='columns', inplace=True)\n",
    "\n",
    "def drop_empty_rows(df):\n",
    "    drop_list = []\n",
    "    for i in range(df.shape[0]):\n",
    "        if df.loc[i,'text'].isspace():\n",
    "            print(\"found empty text @ {}...dropping\".format(i))\n",
    "            drop_list.append(i)\n",
    "    new_df = df.drop(labels=drop_list, axis='index')\n",
    "    new_index = [i for i in range(new_df.shape[0])]\n",
    "    new_df.index = new_index\n",
    "    return new_df\n",
    "\n",
    "df = drop_empty_rows(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6299 texts.\n"
     ]
    }
   ],
   "source": [
    "# prepare text samples and their labels                                                                  \n",
    "texts = list(df['text'])\n",
    "labels_index = {'FAKE': 0, 'REAL': 1}\n",
    "labels = list(df['label'].apply(lambda x: 0 if x == 'FAKE' else 1))\n",
    "\n",
    "print('Found %s texts.' %len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 98817 unique tokens.\n",
      "Shape of data tensor: (6299, 1000)\n",
      "Shape of label tensor: (6299, 2)\n"
     ]
    }
   ],
   "source": [
    "# vectorize the text samples into a 2D integer tensor                                                   \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split the data into a training set and a validation set   \n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val, y_train, y_val = train_test_split(data, labels, test_size=VALIDATION_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare embedding matrix                                                                                       \n",
    "from keras.layers import Embedding\n",
    "from keras.initializers import Constant\n",
    "\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.                                                  \n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer                                                       \n",
    "# note that we set trainable = False so as to keep the embeddings fixed                                          \n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5039 samples, validate on 1260 samples\n",
      "Epoch 1/20\n",
      "5039/5039 [==============================] - 153s 30ms/step - loss: 0.6581 - acc: 0.6670 - val_loss: 0.3850 - val_acc: 0.8389\n",
      "Epoch 2/20\n",
      "5039/5039 [==============================] - 149s 30ms/step - loss: 0.3566 - acc: 0.8478 - val_loss: 0.6819 - val_acc: 0.6405\n",
      "Epoch 3/20\n",
      "5039/5039 [==============================] - 144s 29ms/step - loss: 0.2495 - acc: 0.8962 - val_loss: 0.2404 - val_acc: 0.9151\n",
      "Epoch 4/20\n",
      "5039/5039 [==============================] - 144s 29ms/step - loss: 0.1856 - acc: 0.9325 - val_loss: 0.1954 - val_acc: 0.9302\n",
      "Epoch 5/20\n",
      "5039/5039 [==============================] - 147s 29ms/step - loss: 0.1187 - acc: 0.9595 - val_loss: 0.2171 - val_acc: 0.9270\n",
      "Epoch 6/20\n",
      "5039/5039 [==============================] - 144s 29ms/step - loss: 0.1122 - acc: 0.9609 - val_loss: 0.2442 - val_acc: 0.9190\n",
      "Epoch 7/20\n",
      "5039/5039 [==============================] - 172s 34ms/step - loss: 0.0682 - acc: 0.9811 - val_loss: 0.2742 - val_acc: 0.9238\n",
      "Epoch 8/20\n",
      "5039/5039 [==============================] - 195s 39ms/step - loss: 0.0695 - acc: 0.9843 - val_loss: 0.2864 - val_acc: 0.9294\n",
      "Epoch 9/20\n",
      "5039/5039 [==============================] - 172s 34ms/step - loss: 0.0848 - acc: 0.9857 - val_loss: 0.2786 - val_acc: 0.9206\n",
      "Epoch 10/20\n",
      "5039/5039 [==============================] - 192s 38ms/step - loss: 0.0043 - acc: 0.9996 - val_loss: 0.3574 - val_acc: 0.9270\n",
      "Epoch 11/20\n",
      "5039/5039 [==============================] - 182s 36ms/step - loss: 0.0997 - acc: 0.9875 - val_loss: 0.3395 - val_acc: 0.9278\n",
      "Epoch 12/20\n",
      "5039/5039 [==============================] - 184s 36ms/step - loss: 0.0019 - acc: 0.9996 - val_loss: 0.3859 - val_acc: 0.9302\n",
      "Epoch 13/20\n",
      "5039/5039 [==============================] - 173s 34ms/step - loss: 0.1006 - acc: 0.9802 - val_loss: 0.3243 - val_acc: 0.9310\n",
      "Epoch 14/20\n",
      "5039/5039 [==============================] - 204s 41ms/step - loss: 0.0636 - acc: 0.9865 - val_loss: 0.2940 - val_acc: 0.9254\n",
      "Epoch 15/20\n",
      "5039/5039 [==============================] - 165s 33ms/step - loss: 0.0027 - acc: 0.9996 - val_loss: 0.4125 - val_acc: 0.9262\n",
      "Epoch 16/20\n",
      "5039/5039 [==============================] - 188s 37ms/step - loss: 1.4569e-04 - acc: 1.0000 - val_loss: 0.4792 - val_acc: 0.9270\n",
      "Epoch 17/20\n",
      "5039/5039 [==============================] - 218s 43ms/step - loss: 2.2124e-05 - acc: 1.0000 - val_loss: 0.5788 - val_acc: 0.9270\n",
      "Epoch 18/20\n",
      "5039/5039 [==============================] - 210s 42ms/step - loss: 1.8841e-06 - acc: 1.0000 - val_loss: 0.5869 - val_acc: 0.9278\n",
      "Epoch 19/20\n",
      "5039/5039 [==============================] - 259s 51ms/step - loss: 3.0439e-07 - acc: 1.0000 - val_loss: 0.6489 - val_acc: 0.9262\n",
      "Epoch 20/20\n",
      "5039/5039 [==============================] - 189s 37ms/step - loss: 1.3834e-07 - acc: 1.0000 - val_loss: 0.6659 - val_acc: 0.9230\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x10fe3b7f0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train a 1D convnet with global maxpooling                                                                      \n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.models import Model\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(len(labels_index), activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          validation_data=(x_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  100.00%\n",
      "Testing Accuracy:  92.30%\n"
     ]
    }
   ],
   "source": [
    "# evaluate model\n",
    "\n",
    "def predict(X):\n",
    "    return np.rint(model.predict(X)) # threshold the predictions to retrieve labels\n",
    "\n",
    "evaluate_model(predict, x_train, y_train, x_val, y_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
