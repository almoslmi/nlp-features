{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup our environment\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "DATA_FILE = \"data/fake_or_real_news.csv\"\n",
    "TEST_SIZE = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can't use GPU here -- network too big for syntactic features.\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that allows us to evaluate our models\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def evaluate_model(predict_fun, X_train, y_train, X_test, y_test):\n",
    "    '''\n",
    "    evaluate the model, both training and testing errors are reported\n",
    "    '''\n",
    "    # training error\n",
    "    y_predict_train = predict_fun(X_train)\n",
    "    train_acc = accuracy_score(y_train,y_predict_train)\n",
    "    \n",
    "    # testing error\n",
    "    y_predict_test = predict_fun(X_test)\n",
    "    test_acc = accuracy_score(y_test,y_predict_test)\n",
    "    \n",
    "    return train_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate 95% confidence interval on error\n",
    "\n",
    "# NOTE: based on conversation on stackexchange: \n",
    "# https://stats.stackexchange.com/questions/247551/how-to-determine-the-confidence-of-a-neural-network-prediction\n",
    "# towards bottom of the page.\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "def error_conf(error, n):\n",
    "    term = 1.96*sqrt((error*(1-error))/n)\n",
    "    lb = error - term\n",
    "    ub = error + term\n",
    "    \n",
    "    return lb, ub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in our data and preprocess it\n",
    "# Note: the news items in the data set range from 0 words to 100,000 words\n",
    "# we restrict ourselves to news items between 500 and 5,000 words.\n",
    "\n",
    "df = pd.read_csv(DATA_FILE)\n",
    "df.drop(labels=['id','title'], axis='columns', inplace=True)\n",
    "# only select stories with between 500 and 5000 words\n",
    "mask = list(df['text'].apply(lambda x: len(x) >= 500 and len(x) <= 5000))\n",
    "df = df[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3510, 2)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1472 2038\n"
     ]
    }
   ],
   "source": [
    "# show that the sample data is pretty balanced\n",
    "print(len(df[df['label'] == 'REAL']), len(df[df['label'] == 'FAKE']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text']\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert labels to numeric labels\n",
    "# NOTE: DNNs need numeric labels\n",
    "\n",
    "def convert(x):\n",
    "    if x == 'FAKE':\n",
    "        return 0.0\n",
    "    else:\n",
    "        return 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntactic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up vector models for training and testing\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# data vectorizer\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", \n",
    "                             binary = True, \n",
    "                             min_df = 2,\n",
    "                             stop_words='english')\n",
    "docarray = vectorizer.fit_transform(X).toarray()\n",
    "docterm = pd.DataFrame(docarray, columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3510, 23516)\n"
     ]
    }
   ],
   "source": [
    "print(docterm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "docterm_train, docterm_test, y_train, y_test = train_test_split(docterm, y, test_size=TEST_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes Model - train & test\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB()\n",
    "model.fit(docterm_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 96.62%\n",
      "Testing Accuracy: 88.89%\n"
     ]
    }
   ],
   "source": [
    "# evaluate model\n",
    "train_acc, test_acc = evaluate_model(model.predict, docterm_train, y_train, docterm_test, y_test)\n",
    "print(\"Training Accuracy: {:.2f}%\".format(train_acc*100))\n",
    "print(\"Testing Accuracy: {:.2f}%\".format(test_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% confidence interval: 86.56%-91.21%\n"
     ]
    }
   ],
   "source": [
    "# computing 95% confidence interval\n",
    "n = docterm_test.shape[0]\n",
    "lb, ub = error_conf(1-test_acc, n)\n",
    "\n",
    "print(\"95% confidence interval: {:.2f}%-{:.2f}%\".format((1-ub)*100,(1-lb)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=60,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest Model - train & test\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(min_samples_split=60)\n",
    "model.fit(docterm_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 96.08%\n",
      "Testing Accuracy: 83.76%\n"
     ]
    }
   ],
   "source": [
    "# evaluate model\n",
    "train_acc, test_acc = evaluate_model(model.predict, docterm_train, y_train, docterm_test, y_test)\n",
    "print(\"Training Accuracy: {:.2f}%\".format(train_acc*100))\n",
    "print(\"Testing Accuracy: {:.2f}%\".format(test_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% confidence interval: 81.03%-86.49%\n"
     ]
    }
   ],
   "source": [
    "# computing 95% confidence interval\n",
    "n = docterm_test.shape[0]\n",
    "lb, ub = error_conf(1-test_acc, n)\n",
    "\n",
    "print(\"95% confidence interval: {:.2f}%-{:.2f}%\".format((1-ub)*100,(1-lb)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2808 samples, validate on 702 samples\n",
      "Epoch 1/20\n",
      "2808/2808 [==============================] - 7s 3ms/step - loss: 0.6586 - acc: 0.6503 - val_loss: 0.4126 - val_acc: 0.8761\n",
      "Epoch 2/20\n",
      "2808/2808 [==============================] - 6s 2ms/step - loss: 0.2562 - acc: 0.9120 - val_loss: 0.1766 - val_acc: 0.9387\n",
      "Epoch 3/20\n",
      "2808/2808 [==============================] - 6s 2ms/step - loss: 0.0689 - acc: 0.9825 - val_loss: 0.2625 - val_acc: 0.9288\n",
      "Epoch 4/20\n",
      "2808/2808 [==============================] - 6s 2ms/step - loss: 0.0163 - acc: 0.9968 - val_loss: 0.4506 - val_acc: 0.9231\n",
      "Epoch 5/20\n",
      "2808/2808 [==============================] - 6s 2ms/step - loss: 0.0099 - acc: 0.9975 - val_loss: 0.4665 - val_acc: 0.9345\n",
      "Epoch 6/20\n",
      "2808/2808 [==============================] - 6s 2ms/step - loss: 0.0013 - acc: 0.9996 - val_loss: 0.5281 - val_acc: 0.9345\n",
      "Epoch 7/20\n",
      "2808/2808 [==============================] - 6s 2ms/step - loss: 2.1231e-04 - acc: 1.0000 - val_loss: 0.6426 - val_acc: 0.9316\n",
      "Epoch 8/20\n",
      "2808/2808 [==============================] - 6s 2ms/step - loss: 1.8689e-04 - acc: 1.0000 - val_loss: 0.7708 - val_acc: 0.9316\n",
      "Epoch 9/20\n",
      "2808/2808 [==============================] - 6s 2ms/step - loss: 6.8462e-05 - acc: 1.0000 - val_loss: 0.8270 - val_acc: 0.9345\n",
      "Epoch 10/20\n",
      "2808/2808 [==============================] - 6s 2ms/step - loss: 0.0042 - acc: 0.9993 - val_loss: 0.7483 - val_acc: 0.9345\n",
      "Epoch 11/20\n",
      "2808/2808 [==============================] - 7s 2ms/step - loss: 9.5374e-04 - acc: 0.9996 - val_loss: 0.7564 - val_acc: 0.9316\n",
      "Epoch 12/20\n",
      "2808/2808 [==============================] - 7s 2ms/step - loss: 8.2575e-05 - acc: 1.0000 - val_loss: 0.8483 - val_acc: 0.9330\n",
      "Epoch 13/20\n",
      "2808/2808 [==============================] - 6s 2ms/step - loss: 2.1608e-05 - acc: 1.0000 - val_loss: 0.8824 - val_acc: 0.9330\n",
      "Epoch 14/20\n",
      "2808/2808 [==============================] - 6s 2ms/step - loss: 0.0057 - acc: 0.9996 - val_loss: 0.8361 - val_acc: 0.9345\n",
      "Epoch 15/20\n",
      "2808/2808 [==============================] - 6s 2ms/step - loss: 0.0057 - acc: 0.9996 - val_loss: 1.0126 - val_acc: 0.9245\n",
      "Epoch 16/20\n",
      "2808/2808 [==============================] - 6s 2ms/step - loss: 8.2490e-06 - acc: 1.0000 - val_loss: 1.0891 - val_acc: 0.9231\n",
      "Epoch 17/20\n",
      "2808/2808 [==============================] - 6s 2ms/step - loss: 0.0026 - acc: 0.9993 - val_loss: 0.9480 - val_acc: 0.9316\n",
      "Epoch 18/20\n",
      "2808/2808 [==============================] - 6s 2ms/step - loss: 0.0015 - acc: 0.9996 - val_loss: 0.9500 - val_acc: 0.9345\n",
      "Epoch 19/20\n",
      "2808/2808 [==============================] - 6s 2ms/step - loss: 0.0057 - acc: 0.9996 - val_loss: 0.9207 - val_acc: 0.9345\n",
      "Epoch 20/20\n",
      "2808/2808 [==============================] - 6s 2ms/step - loss: 1.8204e-05 - acc: 1.0000 - val_loss: 0.9524 - val_acc: 0.9316\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f93f6206160>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DNN\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "# build the model\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, input_dim=docterm_train.shape[1], activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(docterm_train, y_train.apply(convert),\n",
    "          epochs=20,\n",
    "          batch_size=128,\n",
    "          validation_data=(docterm_test, y_test.apply(convert)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 100.00%\n",
      "Testing Accuracy: 93.16%\n"
     ]
    }
   ],
   "source": [
    "# evaluate model\n",
    "train_acc, test_acc = evaluate_model(model.predict_classes, \n",
    "                                     docterm_train, \n",
    "                                     y_train.apply(convert), \n",
    "                                     docterm_test, \n",
    "                                     y_test.apply(convert))\n",
    "print(\"Training Accuracy: {:.2f}%\".format(train_acc*100))\n",
    "print(\"Testing Accuracy: {:.2f}%\".format(test_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% confidence interval: 91.30%-95.03%\n"
     ]
    }
   ],
   "source": [
    "# computing 95% confidence interval\n",
    "n = docterm_test.shape[0]\n",
    "lb, ub = error_conf(1-test_acc, n)\n",
    "\n",
    "print(\"95% confidence interval: {:.2f}%-{:.2f}%\".format((1-ub)*100,(1-lb)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Spacy semantic model\n",
    "\n",
    "import spacy\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "# NOTE: for performance reasons disable everything in the pipeline except the tokenizer\n",
    "nlp = spacy.load('en_core_web_lg', disable=['parser', 'tagger', 'ner', 'textcat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3510,)\n"
     ]
    }
   ],
   "source": [
    "# preprocess text for semantic features\n",
    "\n",
    "def embed(X):\n",
    "    '''\n",
    "    x is a list of strings and embed will compute\n",
    "    an embedding vector for each and return an array\n",
    "    of shape (len(x),EMBEDDING_DIM)\n",
    "    '''\n",
    "    vectors = []\n",
    "    text_array = np.array(X)\n",
    "\n",
    "    print(text_array.shape)\n",
    "    \n",
    "    for i in range(text_array.shape[0]):\n",
    "        vector = nlp(text_array[i]).vector\n",
    "        vectors.append(vector)\n",
    "    \n",
    "    return pd.DataFrame(vectors)\n",
    "\n",
    "vectors = embed(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "vectors_train, vectors_test, y_train, y_test = train_test_split(vectors, y, test_size=TEST_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes Model \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "model.fit(vectors_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 73.65%\n",
      "Testing Accuracy: 74.36%\n"
     ]
    }
   ],
   "source": [
    "# evaluate model\n",
    "train_acc, test_acc = evaluate_model(model.predict, vectors_train, y_train, vectors_test, y_test)\n",
    "print(\"Training Accuracy: {:.2f}%\".format(train_acc*100))\n",
    "print(\"Testing Accuracy: {:.2f}%\".format(test_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% confidence interval: 71.13%-77.59%\n"
     ]
    }
   ],
   "source": [
    "# computing 95% confidence interval\n",
    "n = docterm_test.shape[0]\n",
    "lb, ub = error_conf(1-test_acc, n)\n",
    "\n",
    "print(\"95% confidence interval: {:.2f}%-{:.2f}%\".format((1-ub)*100,(1-lb)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=10,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest Model \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(min_samples_split=10)\n",
    "model.fit(vectors_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 98.25%\n",
      "Testing Accuracy: 82.34%\n"
     ]
    }
   ],
   "source": [
    "# evaluate model\n",
    "train_acc, test_acc = evaluate_model(model.predict, vectors_train, y_train, vectors_test, y_test)\n",
    "print(\"Training Accuracy: {:.2f}%\".format(train_acc*100))\n",
    "print(\"Testing Accuracy: {:.2f}%\".format(test_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% confidence interval: 79.52%-85.16%\n"
     ]
    }
   ],
   "source": [
    "# computing 95% confidence interval\n",
    "n = docterm_test.shape[0]\n",
    "lb, ub = error_conf(1-test_acc, n)\n",
    "\n",
    "print(\"95% confidence interval: {:.2f}%-{:.2f}%\".format((1-ub)*100,(1-lb)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNN - MLP\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "# build the model\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=EMBEDDING_DIM, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2808 samples, validate on 702 samples\n",
      "Epoch 1/15\n",
      "2808/2808 [==============================] - 1s 190us/step - loss: 0.6838 - acc: 0.5577 - val_loss: 0.6574 - val_acc: 0.5798\n",
      "Epoch 2/15\n",
      "2808/2808 [==============================] - 0s 43us/step - loss: 0.6375 - acc: 0.6140 - val_loss: 0.5383 - val_acc: 0.7892\n",
      "Epoch 3/15\n",
      "2808/2808 [==============================] - 0s 42us/step - loss: 0.5494 - acc: 0.7301 - val_loss: 0.4254 - val_acc: 0.8376\n",
      "Epoch 4/15\n",
      "2808/2808 [==============================] - 0s 42us/step - loss: 0.4773 - acc: 0.7910 - val_loss: 0.3682 - val_acc: 0.8533\n",
      "Epoch 5/15\n",
      "2808/2808 [==============================] - 0s 45us/step - loss: 0.4330 - acc: 0.8102 - val_loss: 0.3752 - val_acc: 0.8433\n",
      "Epoch 6/15\n",
      "2808/2808 [==============================] - 0s 42us/step - loss: 0.4076 - acc: 0.8237 - val_loss: 0.3080 - val_acc: 0.8746\n",
      "Epoch 7/15\n",
      "2808/2808 [==============================] - 0s 42us/step - loss: 0.3749 - acc: 0.8383 - val_loss: 0.3017 - val_acc: 0.8732\n",
      "Epoch 8/15\n",
      "2808/2808 [==============================] - 0s 43us/step - loss: 0.3726 - acc: 0.8472 - val_loss: 0.2831 - val_acc: 0.8917\n",
      "Epoch 9/15\n",
      "2808/2808 [==============================] - 0s 43us/step - loss: 0.3471 - acc: 0.8625 - val_loss: 0.2757 - val_acc: 0.8946\n",
      "Epoch 10/15\n",
      "2808/2808 [==============================] - 0s 43us/step - loss: 0.3448 - acc: 0.8579 - val_loss: 0.3183 - val_acc: 0.8675\n",
      "Epoch 11/15\n",
      "2808/2808 [==============================] - 0s 42us/step - loss: 0.3190 - acc: 0.8736 - val_loss: 0.2688 - val_acc: 0.8917\n",
      "Epoch 12/15\n",
      "2808/2808 [==============================] - 0s 44us/step - loss: 0.3175 - acc: 0.8789 - val_loss: 0.3379 - val_acc: 0.8462\n",
      "Epoch 13/15\n",
      "2808/2808 [==============================] - 0s 43us/step - loss: 0.3141 - acc: 0.8743 - val_loss: 0.2587 - val_acc: 0.8932\n",
      "Epoch 14/15\n",
      "2808/2808 [==============================] - 0s 42us/step - loss: 0.3030 - acc: 0.8796 - val_loss: 0.2663 - val_acc: 0.8946\n",
      "Epoch 15/15\n",
      "2808/2808 [==============================] - 0s 42us/step - loss: 0.2917 - acc: 0.8882 - val_loss: 0.2511 - val_acc: 0.8989\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f93648e9da0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(vectors_train, y_train.apply(convert),\n",
    "          epochs=15,\n",
    "          batch_size=128,\n",
    "          validation_data=(vectors_test, y_test.apply(convert)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 89.74%\n",
      "Testing Accuracy: 89.89%\n"
     ]
    }
   ],
   "source": [
    "# evaluate model\n",
    "train_acc, test_acc = evaluate_model(model.predict_classes, \n",
    "                                     vectors_train, \n",
    "                                     y_train.apply(convert), \n",
    "                                     vectors_test, \n",
    "                                     y_test.apply(convert))\n",
    "print(\"Training Accuracy: {:.2f}%\".format(train_acc*100))\n",
    "print(\"Testing Accuracy: {:.2f}%\".format(test_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% confidence interval: 87.66%-92.12%\n"
     ]
    }
   ],
   "source": [
    "# estimating 95% confidence interval\n",
    "n = docterm_test.shape[0]\n",
    "lb, ub = error_conf(1-test_acc, n)\n",
    "\n",
    "print(\"95% confidence interval: {:.2f}%-{:.2f}%\".format((1-ub)*100,(1-lb)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
